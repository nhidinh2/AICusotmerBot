"""
QA System Improvement Utilities

This module contains helper functions and classes to improve the QA system.
"""

import asyncio
from typing import Dict, List, Optional, Any
import pandas as pd
from graphrag.query.context_builder.conversation_history import ConversationHistory, ConversationRole


def format_sources_with_context(context_data: Dict[str, pd.DataFrame]) -> str:
    """
    Format source citations in a user-friendly way.
    
    Args:
        context_data: Dictionary containing context data from search result
        
    Returns:
        Formatted string with source citations
    """
    sources_text = []
    
    if "sources" in context_data:
        sources_df = context_data["sources"]
        for idx, row in sources_df.iterrows():
            # Get first 150 characters as preview
            text_preview = row["text"][:150] + "..." if len(row["text"]) > 150 else row["text"]
            sources_text.append(f"[{idx}] {text_preview}")
    
    if "entities" in context_data:
        entities_df = context_data["entities"]
        if len(entities_df) > 0:
            sources_text.append(f"\nC√°c th·ª±c th·ªÉ li√™n quan: {', '.join(entities_df.head(5)['title'].tolist())}")
    
    if sources_text:
        return "\n\nüìö Ngu·ªìn tham kh·∫£o:\n" + "\n".join(sources_text)
    
    return "\n\nüìö Ngu·ªìn tham kh·∫£o: Kh√¥ng c√≥ th√¥ng tin"


async def validate_answer_quality(
    question: str,
    answer: str,
    context_data: Dict[str, pd.DataFrame],
    llm,
) -> Dict[str, Any]:
    """
    Validate answer quality and grounding.
    
    Args:
        question: User question
        answer: Generated answer
        context_data: Context data used for answer
        llm: LLM instance for validation
        
    Returns:
        Dictionary with validation scores and issues
    """
    # Extract source texts
    source_texts = []
    if "sources" in context_data:
        source_texts = context_data["sources"]["text"].tolist()
    
    validation_prompt = f"""
B·∫°n l√† chuy√™n gia ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng c√¢u tr·∫£ l·ªùi. H√£y ƒë√°nh gi√° c√¢u tr·∫£ l·ªùi sau:

C√¢u h·ªèi: {question}
C√¢u tr·∫£ l·ªùi: {answer}
Ngu·ªìn t√†i li·ªáu: {' '.join(source_texts[:3])}

H√£y ƒë√°nh gi√° theo c√°c ti√™u ch√≠ (0-10):
1. T√≠nh ch√≠nh x√°c (Factual Accuracy): C√¢u tr·∫£ l·ªùi c√≥ ch√≠nh x√°c kh√¥ng?
2. T√≠nh ƒë·∫ßy ƒë·ªß (Completeness): C√¢u tr·∫£ l·ªùi c√≥ ƒë·∫ßy ƒë·ªß th√¥ng tin kh√¥ng?
3. T√≠nh li√™n quan (Relevance): C√¢u tr·∫£ l·ªùi c√≥ li√™n quan ƒë·∫øn c√¢u h·ªèi kh√¥ng?
4. T√≠nh d·ª±a tr√™n ngu·ªìn (Source Grounding): C√¢u tr·∫£ l·ªùi c√≥ d·ª±a tr√™n ngu·ªìn t√†i li·ªáu kh√¥ng?
5. Ph√°t hi·ªán ·∫£o t∆∞·ªüng (Hallucination): C√≥ th√¥ng tin kh√¥ng c√≥ trong ngu·ªìn kh√¥ng?

Tr·∫£ v·ªÅ JSON:
{{
    "accuracy": <score 0-10>,
    "completeness": <score 0-10>,
    "relevance": <score 0-10>,
    "grounding": <score 0-10>,
    "hallucination": <score 0-10>,
    "issues": ["list of issues if any"],
    "overall_score": <average score>
}}
"""
    
    try:
        result = await llm(validation_prompt)
        # Parse JSON result
        import json
        validation = json.loads(result.output)
        return validation
    except Exception as e:
        return {
            "accuracy": -1,
            "completeness": -1,
            "relevance": -1,
            "grounding": -1,
            "hallucination": -1,
            "issues": [f"Validation failed: {str(e)}"],
            "overall_score": -1
        }


class ConversationManager:
    """Enhanced conversation management with summarization."""
    
    def __init__(self, max_tokens: int = 12000):
        self.history = ConversationHistory()
        self.max_tokens = max_tokens
        self.entity_tracker = {}  # Track entities mentioned in conversation
    
    def add_turn(self, question: str, answer: str):
        """Add a turn to the conversation."""
        self.history.add_turn(ConversationRole.USER, question)
        self.history.add_turn(ConversationRole.ASSISTANT, answer)
    
    def should_summarize(self, token_encoder) -> bool:
        """Check if conversation history should be summarized."""
        if len(self.history.turns) == 0:
            return False
        
        context, _ = self.history.build_context(
            token_encoder=token_encoder,
            max_tokens=self.max_tokens
        )
        
        tokens = token_encoder.encode(context)
        return len(tokens) > self.max_tokens * 0.8
    
    def summarize_old_turns(self, llm, token_encoder):
        """Summarize old conversation turns to save tokens."""
        if len(self.history.turns) <= 4:
            return
        
        # Get old turns (except last 2)
        old_turns = self.history.turns[:-2]
        summary_prompt = f"""
H√£y t√≥m t·∫Øt cu·ªôc h·ªôi tho·∫°i sau m·ªôt c√°ch ng·∫Øn g·ªçn, gi·ªØ l·∫°i c√°c th√¥ng tin quan tr·ªçng:
{chr(10).join(str(turn) for turn in old_turns)}

T√≥m t·∫Øt:
"""
        
        # This would need async implementation
        # For now, just truncate old turns
        self.history.turns = self.history.turns[-2:]
    
    def get_relevant_entities(self) -> List[str]:
        """Get entities mentioned in conversation."""
        return list(self.entity_tracker.keys())


class HybridSearch:
    """Hybrid search that combines local and global search."""
    
    def __init__(self, local_search, global_search, llm=None):
        self.local_search = local_search
        self.global_search = global_search
        self.llm = llm
    
    async def classify_query(self, query: str) -> str:
        """Classify query as 'local' or 'global'."""
        if not self.llm:
            # Simple heuristic: if query contains entity names, use local
            # Otherwise use global
            if len(query.split()) < 5:
                return "local"
            return "global"
        
        classification_prompt = f"""
Ph√¢n lo·∫°i c√¢u h·ªèi sau:
- "local": C√¢u h·ªèi c·ª• th·ªÉ v·ªÅ m·ªôt th·ª±c th·ªÉ/topic c·ª• th·ªÉ
- "global": C√¢u h·ªèi t·ªïng quan, so s√°nh, ho·∫∑c li√™n quan ƒë·∫øn nhi·ªÅu topics

C√¢u h·ªèi: {query}

Tr·∫£ v·ªÅ ch·ªâ m·ªôt t·ª´: "local" ho·∫∑c "global"
"""
        
        try:
            result = await self.llm(classification_prompt)
            return result.output.strip().lower()
        except:
            return "local"  # Default to local
    
    async def search(self, query: str, **kwargs):
        """Search using hybrid approach."""
        query_type = await self.classify_query(query)
        
        if query_type == "global":
            return await self.global_search.asearch(query, **kwargs)
        else:
            return await self.local_search.asearch(query, **kwargs)


def enhance_response_with_metadata(
    response: str,
    context_data: Dict[str, pd.DataFrame],
    validation_score: Optional[Dict[str, Any]] = None
) -> str:
    """
    Enhance response with metadata, sources, and confidence.
    
    Args:
        response: Original response
        context_data: Context data used
        validation_score: Optional validation scores
        
    Returns:
        Enhanced response with metadata
    """
    enhanced = response
    
    # Add sources
    enhanced += format_sources_with_context(context_data)
    
    # Add confidence if available
    if validation_score and validation_score.get("overall_score", -1) >= 0:
        score = validation_score["overall_score"]
        confidence = "Cao" if score >= 8 else "Trung b√¨nh" if score >= 5 else "Th·∫•p"
        enhanced += f"\n\n‚úÖ ƒê·ªô tin c·∫≠y: {confidence} ({score:.1f}/10)"
        
        if validation_score.get("issues"):
            enhanced += f"\n‚ö†Ô∏è L∆∞u √Ω: {', '.join(validation_score['issues'][:2])}"
    
    return enhanced


async def search_with_fallback(
    query: str,
    local_search,
    global_search,
    max_retries: int = 3
):
    """
    Search with fallback strategy.
    
    Args:
        query: User question
        local_search: Local search instance
        global_search: Global search instance
        max_retries: Maximum retry attempts
        
    Returns:
        Search result with fallback handling
    """
    # Try local search first
    try:
        result = await local_search.asearch(query)
        
        # Check if result is meaningful (has entities/context)
        if result.context_data and len(result.context_data.get("entities", [])) > 0:
            return result
        
        # If no entities found, fallback to global
        raise ValueError("No entities found in local search")
        
    except Exception as e:
        print(f"Local search failed: {e}, trying global search...")
        
        # Fallback to global search
        try:
            return await global_search.asearch(query)
        except Exception as e2:
            # Final fallback: return a helpful error message
            from graphrag.query.structured_search.base import SearchResult
            return SearchResult(
                response="Xin l·ªói, t√¥i kh√¥ng th·ªÉ t√¨m th·∫•y th√¥ng tin v·ªÅ c√¢u h·ªèi n√†y trong t√†i li·ªáu. Vui l√≤ng th·ª≠ l·∫°i v·ªõi c√¢u h·ªèi kh√°c ho·∫∑c di·ªÖn ƒë·∫°t l·∫°i c√¢u h·ªèi.",
                context_data={},
                prompt_tokens=0,
                completion_tokens=0,
            )


def expand_vietnamese_query(query: str) -> str:
    """
    Expand query with Vietnamese synonyms (simple implementation).
    
    Args:
        query: Original query
        
    Returns:
        Expanded query
    """
    # Simple synonym expansion (you could use a Vietnamese wordnet or embedding-based expansion)
    synonyms = {
        "th·ªùi gian": ["gi·ªù", "l√∫c", "khung gi·ªù"],
        "ph√≠": ["chi ph√≠", "gi√°", "l·ªá ph√≠"],
        "giao d·ªãch": ["mua b√°n", "trao ƒë·ªïi", "th·ª±c hi·ªán"],
        "ch·ª©ng kho√°n": ["c·ªï phi·∫øu", "tr√°i phi·∫øu"],
    }
    
    expanded_terms = []
    words = query.lower().split()
    
    for word in words:
        expanded_terms.append(word)
        if word in synonyms:
            expanded_terms.extend(synonyms[word][:1])  # Add first synonym
    
    return " ".join(expanded_terms)

